{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Untitled\"\n",
        "format: html\n",
        "---"
      ],
      "id": "3559e74f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Train and classify from multisource (or project) using featurevector files provided on S3.\n",
        "\n",
        "\"\"\"\n",
        "import json\n",
        "import boto3\n",
        "from botocore.exceptions import ClientError, BotoCoreError\n",
        "import os\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "import boto3\n",
        "import io\n",
        "import csv\n",
        "import json\n",
        "import logging\n",
        "import traceback\n",
        "from operator import itemgetter\n",
        "from botocore.exceptions import NoCredentialsError, ClientError\n",
        "from pathlib import Path\n",
        "from spacer import config\n",
        "from spacer.data_classes import ImageLabels\n",
        "from scripts.docker import runtimes\n",
        "from spacer.tasks import process_job, classify_image, extract_features, train_classifier, classify_features\n",
        "from spacer.storage import load_image, store_image\n",
        "from spacer.messages import JobMsg, DataLocation, ExtractFeaturesMsg\n",
        "from spacer.extract_features import EfficientNetExtractor\n",
        "from spacer.messages import (\n",
        "    DataLocation,\n",
        "    ExtractFeaturesMsg, \n",
        "    ExtractFeaturesReturnMsg, \n",
        "    TrainClassifierMsg, \n",
        "    TrainClassifierReturnMsg, \n",
        "    ClassifyFeaturesMsg, \n",
        "    ClassifyImageMsg, \n",
        "    ClassifyReturnMsg, JobMsg, JobReturnMsg\n",
        ")\n",
        "\n",
        "from spacer.tasks import classify_features, extract_features, train_classifier\n",
        "\n",
        "# Load the secret.json file\n",
        "with open('secrets.json', 'r') as f:\n",
        "    secrets = json.load(f)"
      ],
      "id": "f7040a9e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Assign the secrets to boto3\n"
      ],
      "id": "64cfd2b7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "try:\n",
        "    # Load the secret.json file\n",
        "    with open('secrets.json', 'r') as f:\n",
        "        secrets = json.load(f)\n",
        "\n",
        "    # Create a session using the credentials from secrets.json\n",
        "    s3_client = boto3.client(\n",
        "        's3',\n",
        "        region_name=secrets['AWS_REGION'],\n",
        "        aws_access_key_id=secrets['AWS_ACCESS_KEY_ID'],\n",
        "        aws_secret_access_key=secrets['AWS_SECRET_ACCESS_KEY']\n",
        "    )\n",
        "except (ClientError, BotoCoreError) as e:\n",
        "    print(f\"An AWS error occurred: {e}\")\n",
        "except json.JSONDecodeError as e:\n",
        "    print(f\"Error reading secrets.json: {e}\")\n",
        "except IOError as e:\n",
        "    print(f\"File error: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ],
      "id": "34c437e6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Coralnet\n",
        "\n",
        "Let's use three sources that are smaller in size for mvp and store them in a list called `sources`:\n",
        "\n",
        "```         \n",
        "- `s1970`\n",
        "- `s2083`\n",
        "- `s2170`\n",
        "```\n"
      ],
      "id": "a60188d4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sources = ['s1970', 's2083', 's2170']"
      ],
      "id": "5a753709",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Find the annotations.csv files for the chosen sources\n",
        "\n",
        "-   Create the key value from the source\n"
      ],
      "id": "668581b4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create a list for the chosen sources for the annotations f'coralnet_public_features/{source}/annotations.csv'\n",
        "chosen_sources = []\n",
        "for source in sources:\n",
        "    chosen_sources.append(f'coralnet_public_features/{source}/annotations.csv')"
      ],
      "id": "d0c03227",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use S3 to download the annotations.csv files for the chosen sources - check if they exist in the bucket first\n"
      ],
      "id": "b3a779c9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# See if chosen_sources are in the s3 bucket using s3_client\n",
        "bucketname='coralnet-mermaid-share'\n",
        "for source in chosen_sources:\n",
        "    try:\n",
        "        s3_client.head_object(Bucket=bucketname, Key=source)\n",
        "        print(f\"{source} exists in the bucket.\")\n",
        "    except Exception as e:\n",
        "        print(f\"{source} does not exist in the bucket. Error: {e}\")"
      ],
      "id": "df5a6cd5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Append annotations\n",
        "\n",
        "-   The following code will append the annotations.csv files for the chosen sources into one dataframe called `appended_df` in chunks that you can specify. In the future for larger sources we may need to consider memory efficient ways of combining the annotations.csv files.\n",
        "\n",
        "Using pandas only option for now. Other options include: - dask - sqllite - pyarrow/parquet\n"
      ],
      "id": "0d62849d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def read_csv_in_chunks(bucketname, key, chunksize=10000):\n",
        "    \"\"\"\n",
        "    Read a CSV file from S3 in chunks.\n",
        "    Append the source as a column called 'source_id'\n",
        "    \"\"\"\n",
        "\n",
        "    response = s3_client.get_object(Bucket=bucketname, Key=key)\n",
        "    lines = []\n",
        "    header = None\n",
        "    for line in response['Body'].iter_lines():\n",
        "        if not header:\n",
        "            header = line.decode('utf-8')\n",
        "            continue\n",
        "        lines.append(line.decode('utf-8'))\n",
        "        if len(lines) == chunksize:\n",
        "            chunk = pd.read_csv(io.StringIO(header + '\\n' + '\\n'.join(lines)))\n",
        "            chunk['source_id'] = key  # Add the source_id column\n",
        "            yield chunk\n",
        "            lines = []\n",
        "    if lines:\n",
        "        chunk = pd.read_csv(io.StringIO(header + '\\n' + '\\n'.join(lines)))\n",
        "        chunk['source_id'] = key  # Add the source_id column\n",
        "        yield chunk\n"
      ],
      "id": "f3d8e5b7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Append each key in chosen_sources to the appended_df dataframe for each source.\n"
      ],
      "id": "84589dcb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "appended_df = pd.DataFrame()\n",
        "\n",
        "for key in chosen_sources:\n",
        "        first_chunk = True\n",
        "        for chunk in read_csv_in_chunks(bucketname, key, chunksize=10000):\n",
        "            if first_chunk:\n",
        "                if appended_df.empty:\n",
        "                    appended_df = chunk\n",
        "                else:\n",
        "                    if not chunk.columns.equals(appended_df.columns):\n",
        "                        raise ValueError(f\"Inconsistent data structure in file: {key}\")\n",
        "                first_chunk = False\n",
        "            appended_df = pd.concat([appended_df, chunk], ignore_index=True)"
      ],
      "id": "9eed56e5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create a key to download the features from S3 given:\n",
        "# 'coralnet_public_features/{source_id}/features/i{image_id}'\n",
        "# E.g. 'coralnet_public_features/s1073/features/i84392.featurevector'\n",
        "# Where source_id column currently looks like 'coralnet_public_features/s1073/annotations.csv'\n",
        "# and image_id column currently looks like '84392'\n",
        "# and the features are stored in the features directory\n",
        "\n",
        "def format_featurevector_key(image_id, source_id):\n",
        "    \"\"\"\n",
        "    Format the featurevector key to include the directories we need to access in S3.\n",
        "    E.g. 'coralnet_public_features/{source_id}/features/i{image_id}'\n",
        "    \"\"\"\n",
        "    source_id = source_id.split('/')[1]  # Get the source_id from the source_id column\n",
        "    image_id = 'i' + str(image_id) + '.featurevector'  # Format the image_id\n",
        "    return 'coralnet_public_features/' + source_id + '/features/' + image_id"
      ],
      "id": "5cad8258",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Apply the format_featurevector_key function to the appended_df dataframe to create a new column called key.\n"
      ],
      "id": "02b7d1d5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Format new column in appended_df called key\n",
        "appended_df['key'] = appended_df.apply(lambda x: format_featurevector_key(x['Image ID'], x['source_id']), axis=1)"
      ],
      "id": "d22213b1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Data\n",
        "\n",
        "-   This is a basic implementation on how to split the train and val labels by a 7:1 ratio. However, we may want to consider other ways of splitting the data in the future.\n"
      ],
      "id": "b80c3c30"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# CoralNet uses a 7-to-1 ratio of train_labels to val_labels.\n",
        "# Calculate the split index\n",
        "total_labels = len(appended_df)\n",
        "train_size = int(total_labels * 7 / 8)  # 7 parts for training out of 8 total parts\n",
        "\n",
        "# Split the data\n",
        "train_labels_data = appended_df.iloc[:train_size]\n",
        "val_labels_data = appended_df.iloc[train_size:]"
      ],
      "id": "4ec443c0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   To work with the `ImageLabels` class, we need to convert the train_labels_data and val_labels_data to the required format which is represented as a dictionary of lists of tuples. The dictionary keys are the image keys, and the values are lists of tuples of the form (row, column, label_id).\n"
      ],
      "id": "7ea637b7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Convert train_labels_data and val_labels_data to the required format\n",
        "train_labels_data = {f\"{key}\": [tuple(x) for x in group[['Row', 'Column', 'Label ID']].values]\n",
        "                     for key, group in train_labels_data.groupby('key')}\n",
        "train_labels_data"
      ],
      "id": "a3e2ccd4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "val_labels_data = {f\"{key}\": [tuple(x) for x in group[['Row', 'Column', 'Label ID']].values]\n",
        "                   for key, group in val_labels_data.groupby('key')}"
      ],
      "id": "2a9c8221",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Use spacer to create train classifier msg\n"
      ],
      "id": "e7a36dae"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "train_msg = TrainClassifierMsg(\n",
        "    job_token='mulitest',\n",
        "    trainer_name='minibatch',\n",
        "    nbr_epochs=10,\n",
        "    clf_type='MLP',\n",
        "    # A subset\n",
        "    train_labels=ImageLabels(data = train_labels_data),\n",
        "    val_labels=ImageLabels(data = val_labels_data),\n",
        "    #S3 bucketname\n",
        "    features_loc=DataLocation('s3', bucketname = bucketname, key=''),\n",
        "    previous_model_locs=[],\n",
        "    model_loc=DataLocation('filesystem',str(Path.cwd())+'/classifier_test.pkl'),\n",
        "    valresult_loc=DataLocation('filesystem',str(Path.cwd())+'/valresult_test.json'),\n",
        "\n",
        ")"
      ],
      "id": "1d49d0b3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "return_msg = train_classifier(train_msg)"
      ],
      "id": "a96935d2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "classifier_filepath = Path.cwd() /'classifier_test.pkl'\n",
        "valresult_filepath = Path.cwd()  / 'valresult_test.json'"
      ],
      "id": "d859c3cf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ref_accs_str = \", \".join([f\"{100*acc:.1f}\" for acc in return_msg.ref_accs])\n",
        "\n",
        "print(\"------------------------------\")\n",
        "print(f\"Classifier stored at: {classifier_filepath}\")\n",
        "print(f\"New model's accuracy: {100*return_msg.acc:.1f}%\")\n",
        "print(\n",
        "    \"New model's accuracy progression (calculated on part of train_labels)\"\n",
        "    f\" after each epoch of training: {ref_accs_str}\")\n",
        "\n",
        "print(f\"Evaluation results:\")\n",
        "with open(valresult_filepath) as f:\n",
        "    valresult = json.load(f)"
      ],
      "id": "4ccde51e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "------------------------------------------------------------------------\n",
        "\n",
        "## Create matching labels between coralnet and mermaid\n"
      ],
      "id": "a70a975a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "label_shortcode = pd.read_csv('coral_net_mermaid_labels.csv')\n",
        "\n",
        "# Rename ID to class and Default shord code to shortcode\n",
        "label_shortcode = label_shortcode.rename(columns={'ID': 'classes', 'Default short code': 'shortcode'})\n",
        "\n",
        "# Get the unique class and shortcode pairs\n",
        "label_shortcode = label_shortcode[['classes', 'shortcode']].drop_duplicates()"
      ],
      "id": "ca6b48fb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Convert label_shortcode DataFrame to a dictionary\n",
        "# Create label_list\n",
        "# Account for those that don't match with a default shortcode\n",
        "label_list = [label_ids_to_shortcodes.get(label_id, 'Unknown') for label_id in valresult['classes']]"
      ],
      "id": "ab285067",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for ground_truth_i, prediction_i, score in zip(\n",
        "    valresult['gt'], valresult['est'], valresult['scores']\n",
        "):\n",
        "    print(f\"Actual = {label_list[ground_truth_i]}, Predicted = {label_list[prediction_i]}, Confidence = {100*score:.1f}%\")\n",
        "\n",
        "print(f\"Train time: {return_msg.runtime:.1f} s\")"
      ],
      "id": "107b2bf3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# From chosen_prj, get the features for the source from the features directory from the full Key path\n",
        "# E.g. coralnet_public_features/s1073/features/i84392.featurevector\n",
        "# Use Regex to get the features\n",
        "feature_files = chosen_prj[chosen_prj['Key'].str.contains(r'coralnet_public_features/.*/features/.*\\.featurevector$')]\n"
      ],
      "id": "06e80250",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## For each feature file, create a classify features message\n"
      ],
      "id": "8ad386f6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create a list of classify features messages\n",
        "# For each feature file, create a classify features message\n",
        "messages = []\n",
        "for key in appended_df['key']:\n",
        "    message = ClassifyFeaturesMsg(\n",
        "        job_token=key,\n",
        "        feature_loc=DataLocation('s3', key=key, bucketname = 'coralnet-mermaid-share'),\n",
        "        classifier_loc=DataLocation('filesystem', classifier_filepath),\n",
        "    )\n",
        "    messages.append(message)\n",
        "    return_msg = classify_features(message)\n",
        "    print(\"------------------------------\")\n",
        "    print(f\"Classification result for {key}:\")\n",
        "\n",
        "    label_ids = return_msg.classes\n",
        "    for i, (row, col, scores) in enumerate(return_msg.scores):\n",
        "        top_scores = sorted(\n",
        "            zip(label_ids, scores), key=itemgetter(1), reverse=True)\n",
        "        top_scores_str = \", \".join([\n",
        "            f\"{label_ids_to_codes[str(label_id)]} = {100*score:.1f}%\"\n",
        "            for label_id, score in top_scores[:TOP_SCORES_PER_POINT]\n",
        "        ])\n",
        "        print(f\"- Row {row}, column {col}: {top_scores_str}\")\n",
        "\n",
        "    print(f\"Classification time: {return_msg.runtime:.1f} s\")\n",
        "\n",
        "print(\"------------------------------\")\n",
        "print(\"Clear the output dir before rerunning this script.\")"
      ],
      "id": "32793d85",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "return_message = classify_features(message)"
      ],
      "id": "00b272b2",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}